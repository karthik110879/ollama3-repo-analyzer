# Server Configuration
PORT=3000
NODE_ENV=development

# Database Configuration (for future use)
# DB_HOST=localhost
# DB_PORT=5432
# DB_NAME=ai_repo_analyzer
# DB_USER=your_username
# DB_PASSWORD=your_password

# API Keys (for future use)
# GITHUB_TOKEN=your_github_token
# OPENAI_API_KEY=your_openai_api_key

# CORS Configuration
CORS_ORIGIN=http://localhost:4200

# GitHub token for higher rate limits and private repos (optional)
# GITHUB_TOKEN=ghp_your_token

# Ollama settings (ensure Ollama is running locally)
OLLAMA_BASE_URL=http://localhost:11434

# Multi-Agent Configuration
# ========================

# Repository Analyzer Agent - SPECIALIZED CODE ANALYSIS MODEL
# Uses CodeLlama 34B for deep architectural analysis and code understanding
# This is the ONLY agent that uses CodeLlama - all others use Ollama3
OLLAMA_ANALYZER_MODEL=codellama:13b
OLLAMA_ANALYZER_TEMPERATURE=0.1

# Mermaid Diagram Builder Agent - uses Ollama3 for creative diagram generation
OLLAMA_DIAGRAM_MODEL=llama3
OLLAMA_DIAGRAM_TEMPERATURE=0.3

# Codebase Chunking Agent - uses Ollama3 for repository chunking
OLLAMA_CHUNKING_MODEL=llama3
OLLAMA_CHUNKING_TEMPERATURE=0.1

# Results Aggregator Agent - uses Ollama3 for combining chunk results
OLLAMA_AGGREGATOR_MODEL=llama3
OLLAMA_AGGREGATOR_TEMPERATURE=0.2

# Chunking Configuration
CHUNKING_THRESHOLD=500
MAX_CHUNK_SIZE=200
MAX_CHUNKS=10
MIN_CHUNK_SIZE=50

# Parallel Processing Configuration
MAX_CONCURRENT_CHUNKS=3
CHUNK_TIMEOUT=120000
RETRY_ATTEMPTS=2

# Fallback model (if specific models not available)
OLLAMA_MODEL=llama3

# Git clone and listing limits
GIT_CLONE_DEPTH=1
FILE_LIST_LIMIT=800
